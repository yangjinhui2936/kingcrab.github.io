# Lecture 3: RNNs

## Video

<iframe width="560" height="315" src="https://www.youtube.com/embed/2b0TPDmzoaQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Slides

<iframe src="//www.slideshare.net/slideshow/embed_code/key/6LPefz3BsAZk6G" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>

[Download slides as PDF](https://drive.google.com/file/d/1bn801i0Brs2FypxDyjBIzJyrNYQof80J/view?usp=sharing)

## Notes

*Lecture by [Josh Tobin](http://josh-tobin.com).*

- 00:00 - Introduction
- 01:34 - Sequence Problems
- 06:28 - Review of RNNs
- 22:00 - Vanishing Gradient Issue
- 27:52 - LSTMs and Its Variants
- 34:10 - Bidirectionality and Attention from Google's Neural Machine Translation
- 46:38 - CTC Loss
- 52:12 - Pros and Cons of Encoder-Decoder LSTM Architectures
- 54:55 - WaveNet
