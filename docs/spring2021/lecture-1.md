# Lecture 1: DL Fundamentals

## Video

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/fGxWfEuUu0w" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Slides

<iframe src="//www.slideshare.net/slideshow/embed_code/key/36tseqKjcKlQwg" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>

[Download slides as PDF](https://drive.google.com/file/d/1Cc3oN9gQSTYPmT7HC7UDaeFXiyJuQwq_/view?usp=sharing)

## Notes

*Lecture by [Sergey Karayev](https://sergeykarayev.com).*

In this video, we discuss the fundamentals of deep learning. We will cover artificial neural networks, the universal approximation theorem, three major types of learning problems, the empirical risk minimization problem, the idea behind gradient descent, the practice of back-propagation, the core neural architectures, and the rise of GPUs.

This should be a review for most of you; if not, then briefly go through this online book -neuralnetworksanddeeplearning.com.

- 1:25​ - Neural Networks
- 6:48​ - Universality
- 8:48​ - Learning Problems
- 16:17​ - Empirical Risk Minimization / Loss Functions
- 19:55​ - Gradient Descent
- 23:57​ - Backpropagation / Automatic Differentiation
- 26:09​ - Architectural Considerations
- 29:01​ - CUDA / Cores of Compute
